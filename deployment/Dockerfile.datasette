# This Dockerfile uses docker multi stage build to avoid having big image.
# https://docs.docker.com/build/building/multi-stage/
#
# To build only the "builder" stage, one can do:
# docker build --target=builder --progress=plain -f deployment/Dockerfile.datasette .


FROM python:3.12-slim-bookworm AS uv_setup

WORKDIR /build

RUN apt-get update && \
    apt-get install -y --no-install-recommends sqlite3 curl ca-certificates

# uv
ADD https://astral.sh/uv/0.7.17/install.sh /uv-installer.sh
RUN sh /uv-installer.sh && rm /uv-installer.sh
ENV PATH="/root/.local/bin/:$PATH"

# -----------------------------------------------------------------------------

FROM uv_setup AS builder

WORKDIR /build

#  Note: docker caches image building so uncomment and change next line
#  when you change tasks.py or data to make sure they are pulled fresh into image
# RUN echo cache busting!

COPY pyproject.toml .python-version uv.lock tasks.py ./
COPY data /build/data

RUN uv sync --group build --locked

RUN ls -al /build/

RUN uv run invoke create-databases --no-validate-arch=aarch64

# -----------------------------------------------------------------------------

FROM uv_setup

RUN apt-get update && \
    apt-get install -y sqlite3

WORKDIR /datasette

COPY pyproject.toml .python-version uv.lock ./

RUN uv sync --locked

COPY --from=builder /build/var/sqlite /datasette/sqlite



EXPOSE 8001
# TODO: remove custom setting when no longer needed
CMD uv run datasette serve --host 0.0.0.0 --port 8001 --cors --inspect-file sqlite/inspect-data.json --metadata sqlite/metadata.json ./sqlite/*.db  --setting max_returned_rows 150000
